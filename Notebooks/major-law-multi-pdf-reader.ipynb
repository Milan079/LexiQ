{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \\\n    transformers \\\n    peft \\\n    huggingface_hub \\\n    accelerate \\\n    bitsandbytes \\\n    sentence-transformers \\\n    langchain \\\n    langchain-community \\\n    PyPDF2\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-16T18:22:59.154444Z","iopub.execute_input":"2025-05-16T18:22:59.154684Z","iopub.status.idle":"2025-05-16T18:24:43.851946Z","shell.execute_reply.started":"2025-05-16T18:22:59.154666Z","shell.execute_reply":"2025-05-16T18:24:43.851111Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install faiss-cpu --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T18:34:35.043769Z","iopub.execute_input":"2025-05-16T18:34:35.044456Z","iopub.status.idle":"2025-05-16T18:34:40.410804Z","shell.execute_reply.started":"2025-05-16T18:34:35.044430Z","shell.execute_reply":"2025-05-16T18:34:40.410073Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport gc\n\ngc.collect()\ntorch.cuda.empty_cache()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nfrom huggingface_hub import login\nfrom PyPDF2 import PdfReader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\n\nlogin(token=\"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\")\n\ndef load_model_and_tokenizer():\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\"NishKook/legal-qa-lora\", use_auth_token=True)\n\n    base_model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.2\",\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        quantization_config=bnb_config,\n        use_auth_token=True,\n        max_memory={0: \"13GiB\", \"cpu\": \"12GiB\"}\n    )\n\n    model = PeftModel.from_pretrained(\n        base_model,\n        \"NishKook/legal-qa-lora\",\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        use_auth_token=True\n    )\n\n    model.eval()\n    return model, tokenizer, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel, tokenizer, device = load_model_and_tokenizer()\n\ndef extract_text_from_multiple_pdfs(pdf_paths):\n    full_text = \"\"\n    for path in pdf_paths:\n        reader = PdfReader(path)\n        for page in reader.pages:\n            if page.extract_text():\n                full_text += page.extract_text() + \"\\n\"\n    return full_text\n\ndef build_vector_index(text):\n    splitter = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=32)\n    docs = [Document(page_content=chunk) for chunk in splitter.split_text(text)]\n    return FAISS.from_documents(docs, HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n\ndef get_context(question, vectordb, k=7):\n    top_docs = vectordb.similarity_search(question, k=k)\n    return \"\\n\".join([doc.page_content for doc in top_docs])\n\ndef generate_answer(question, context):\n    prompt = f\"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=False\n        )\n    return tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\ndef answer_from_pdfs(pdf_paths, question):\n    print(\" Reading multiple PDFs...\")\n    full_text = extract_text_from_multiple_pdfs(pdf_paths)\n\n    print(\" Building FAISS index...\")\n    vectordb = build_vector_index(full_text)\n\n    print(f\" Question: {question}\")\n    context = get_context(question, vectordb)\n\n    print(\"\\n Retrieved Context Snippet:\\n\")\n    print(context[:500], \"...\\n\")\n\n    print(\" Generating answer...\\n\")\n    return generate_answer(question, context)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T18:31:48.234029Z","iopub.execute_input":"2025-05-16T18:31:48.234635Z","iopub.status.idle":"2025-05-16T18:33:14.793753Z","shell.execute_reply.started":"2025-05-16T18:31:48.234613Z","shell.execute_reply":"2025-05-16T18:33:14.793223Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898d99455c324d8abd3723655c2b36de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c81b6dbfc54d52936e3fcc55450835"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c93d0cef9c43d28750cc7094a84ec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/643 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159cdb21f5984780be420d25fda12684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97ff50949cb4495bd02b3ed2e7a6a8a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001379f4ea334d788da8fb2f1fe01e76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c9cf260cf0f47cf8227a707fea1e0a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d1c93d199ab41219df0ca03828c23e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb05ae1c6d4b470fae14fbaf25f1a3c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4b69ffd9ae41df86061123f7412187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73855c883c964259a4b3a48eeebc247a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d39a55b5c0e4240aeb911b5ad312526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de05c2fe3ab945a4af892b21c104531e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a56d121b454659b4507c4888301685"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d33417da5f4d14881b6f472e1b393d"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"pdfs = [\"/kaggle/input/obergefell-v-hodges/Department of Justice Brief.pdf\", \"/kaggle/input/obergefell-v-hodges/Majority Opinion.pdf\", \"/kaggle/input/obergefell-v-hodges/Sumary.pdf\"]\nquestion = \"What were the main disagreements between the majority opinion and the dissenting justices in Obergefell v. Hodges?\"\nanswer = answer_from_pdfs(pdfs, question)\nprint(\" Final Answer:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T18:34:46.205810Z","iopub.execute_input":"2025-05-16T18:34:46.206104Z","iopub.status.idle":"2025-05-16T18:35:02.824830Z","shell.execute_reply.started":"2025-05-16T18:34:46.206079Z","shell.execute_reply":"2025-05-16T18:35:02.824230Z"}},"outputs":[{"name":"stdout","text":" Reading multiple PDFs...\n Building FAISS index...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" Question: What were the main disagreements between the majority opinion and the dissenting justices in Obergefell v. Hodges?\n\n Retrieved Context Snippet:\n\n*Bold sentences give the big idea of the ex cerpt and are not a part of the primary source.  \n \n \n \n \nObergefell  v. Hodges  \nU.S. Supreme Court Opinion and Dissents Summary  \n \nThe Supreme Court (Justice Kennedy writing for himself and Justices Ginsburg, Breyer, \nSotomayor, and Kagan) held that the right to marry is a fundamental right that is inherent in the\nROBERT A. KOCH  \nABBY C. WRIGHT  \nJEFFREY E. SANDBERG  \nAttorneys  \nMARCH  2015 \n \nCONSTITUTION 101  \nModule 14: Battles for Freedom and  ...\n\n Generating answer...\n\n Final Answer:\n The main disagreements between the majority opinion and the dissenting justices in Obergefell v. Hodges were over the interpretation of the Fourteenth Amendment's Due Process Clause and Equal Protection Clause. The majority held that the right to marry is a fundamental right under the Due Process Clause, while the dissenting justices disagreed. The majority also held that same-sex couples are entitled to equal protection under the Fourteenth Amendment, while the dissenting justices disagreed with this as well. The majority opinion was written by Justice Kennedy, while the dissenting opinions were written by Chief Justice Roberts, joined by Justices Scalia and Thomas. The dissenting justices criticized the majority for usurping legislative authority and exercising not judgment but will.\n","output_type":"stream"}],"execution_count":7}]}